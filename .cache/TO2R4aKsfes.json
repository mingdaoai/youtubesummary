{"transcript": "Were there any specific mindset shifts\nthat led you to become you know a leader\nin AI from just a developer? I think the\nbig one is understanding that I have\nto Jason is one of the top AI\nconsultants in the world. He worked with\nmajor companies like Zapier and Hopspot\non their AI strategies and his library\ninstructor is now used by some of the\nlargest organizations in the world. It's\ndownloaded over 2.6 6 million times per\nmonth and it completely changed how we\nthink about AI agents today. I\npersonally use it to build my own\nframework and I'm sure many people did\nas well. So first Jason, what are we\nhoping someone will get out of this\nepisode if they watch until the end?\nYeah, generally it's the case that these\nAI agents that we think about, we should\nthink about them in terms of a portfolio\nof tools and all the work that we need\nto do and all the kinds of investments\nwe need is about building out this\nportfolio. Can I do data analysis to add\nnew tools? How do I verify the tools are\ndoing what they can and cannot do? And\nif we combine them together, then we can\nget very valuable and economically\nvaluable AI agents. How do you define AI\nagents and what makes them different\nfrom traditional LLM based applications?\nYeah, I think now for the most part I\njust see AI agents as an LLM with a\nportfolio of tools. And not only is that\ngoing to be the case, but you can often\ntimes add or remove tools or add\ndifferent context to make the AI sort of\nbehave the right way. But I mostly think\nof it as just a tool caller in a for\nloop. I think honestly the for loop\nmight not be necessary in the future.\nRight now the for loop does make sense\nbecause the agents just can't execute\nextended tasks but as we can see on this\nresearch we'll probably show it on the\nscreen the length of tasks that the AI\ncan execute autonomously is growing\nexponentially. So now you know like 03\nis already at like 1.7 hours of\nautonomous just task execution which\nmeans that we might not actually need\nthe for loop when you know using 03 for\nAI agents. What do you think about that?\nYeah, I guess in my mind like just the\ncalling a tool getting the results like\nthat is the for loop. So I think we're\nactually totally in agreement in terms\nof like what this looks like. So can you\nshare any specific practices that you've\ndiscovered for making agents reliable in\nproduction? Yeah, I think it would go\nback down to this like topic analysis,\nright? This this clustering technique\ncuz really what's going to happen is two\nthings. One, you're going to come up\nwith some kinds of\neval like maybe this is good, maybe this\nis bad. I don't really know what 67\nmeans. But if you can cut up your\nconversations or cut up your agents in\nthese different clusters like questions\nabout finances or questions about\nscheduling or answers that require a lot\nof tables, now you have all these little\ndifferent groups and you can compute the\nsame score for each group. And now\nyou'll see, oh, this one is really,\nreally low. This one is really, really\nhigh. Clearly, I should look at whatever\nis low and fix that problem. And let me\nlook at the conversations. Let's look\nat, you know, the tools it uses and ask\nmyself, is this eval low because it's\nreally impossible? Is it just because\nI'm missing some piece of context or\nsome some tool that I I know how to\nbuild? And you just plop it back in. You\nsee those numbers go up and then you\njust sort of do that forever. So\nbasically just setting up the evals from\nthe start and just tracking every metric\nthat you can track. Yeah. But most\nimportantly, grouping things together so\nyou actually know how to cut them up,\nright? If someone just tells you your\nmarketing is bad, it's really hard to\nfigure out what to do. But if someone\nsaid, \"Hey, your marketing does really\nwell for people under 25 and really bad\nfor people over 25.\" Now you can make a\ndecision. Do I want to double down on my\nyounger audience or do I need to change\nmy content for an older audience? Right.\nNow that leads to an action.\nInteresting. So just basically like\nadding more dimensions exactly to the\nmetrics that you check. And what are\nsome of the most common pitfalls that\ndevelopers or businesses face when it\ncomes to deploying agents in production?\nI think a lot of it is just this belief\nthat the agent can do anything and the\nagent should be able to do everything.\nThe joke that I always tell is the more\nI think a company thinks that they need\na smarter AI, the dumber the team is,\nright? Because what it's saying is, oh,\nI don't want to do any thinking about\nwhat my customer wants. I don't want to\ndo any thinking about how my team should\noperate. I want the AI to do it. Whereas\na really great founder usually says,\nI've talked to a lot of customers and\nthis task is really important. I watched\nexperts do this task. It looks like they\ndo it this way. Can my team model out\nthis pipeline? The answer is usually\nyes. And they do quite a good job. Is\nthere any specific like tech stack that\nyou're using to deploy agents in\nproduction? Is there any frameworks that\nyou can recommend? Yeah, not on my end.\nMostly because I let the company sort of\npick the stack and usually the companies\nthat reach out to me already use some\nkind of technology whether it's like a\nyou know Lang chain or like Pyantic for\nexample. But for the most part the most\nimportant thing is to build something\nquickly, start tracking that data and\nthen doing that data analysis. Right? If\nit takes you 6 months to build the app,\nit just means that it takes you six\nmonths to make a better decision using\nthat data. I would rather someone make\nan app as quickly as possible and then\nspend two or three months looking at\nthat data. So just getting the MVP out\nas quickly as possible and then\ncollecting some real feedback and\nimproving better based on that feedback,\nright? Yeah. Like the biggest\nimprovements will always be these like\nweird things that happen. Like we\nlaunched a company once, we launched an\nagent, all of a sudden it stopped doing\nwell and we looked at why it's like oh\nall the people using it are from like\nTurkey and China. We have a multilingual\nproblem, right? And then we spent like a\nmonth basically figuring out how to like\nbuild different UIs and like showing\nlike Chinese and Turkish on the website.\nLike those have become the real\nproblems, not the agent. So it seems\nlike you work with a ton of early\nstartups in AI that are trying to\ndistribute AI agents across the world.\nSo how do you actually productize an\nagent? How do you make it usable by\nanyone around the world? I mean, I think\na lot of it really is just the fact that\nif you try to build something for\neveryone, it's still very hard. And\nusually when I see companies do that, it\ntend to stay away. It's often the case\nthat, you know, when you work with these\nvertically integrated companies, you\nknow what the useful work is. You have\nexperts that you can talk to and\ninterview. You can hire people who do\nthese things themselves and work on\nreplacing the difficult parts of their\njob. That's usually when things are\nreally successful. I think it's much\nharder to like build a good chatbot and\nnot be graphic or open AI. Yeah, I would\nprobably not want to compete with\nOpenAI. Another use case that's\ncurrently becoming more and more popular\nis software development. Like OpenAI\nsays that, you know, right now they have\ntop 50 programmer. I think OT is top 50\nand by the end of this year they're\ngoing to have number one programmer in\nthe world. All of the tech cos are\nsaying that you know EI is already\nresponsible for writing 20 to 30% of\ntheir code. So what do you think um\nabout the role of a developer in the\nfuture? How do you think it's going to\nchange and how can we as developers\nadapt to these changes? Yeah, I mean the\nbiggest one I think is just the fact\nthat distribution matters more than\never. There's only two problems, you\nknow, distribution and product. If\nproduct is made much easier with AI,\nthen I think a lot of the time that you\nsave might be worth going to doing\ndistribution and learning to market\nyourself. And at least if you're\nthinking about doing a business and then\nwhen it comes to actually being a\ndeveloper, a lot of it is now about what\nis the right organization of code? How\ndo I support like high quality\ndocumentation and how do I make sure\nthose are in sync? I basically just had\ncursor and cloud code write I think like\n6,000 lines of docs for this library\nthat I've been working on. How do I\nwrite code in a way that makes it easy\nfor an agent to work with? You know, is\nthat multiple files? Is that single\nfiles? I've changed the way I write\ndocuments where documents now always\nmention the code file so they know where\nto look. Oh, cool. So, we will\ndefinitely get into rack in a bit. I\nthink there are a lot of things you can\nshare on rack and on how to improve rack\nsystems. But first of all, I wanted to\ntalk about a video that I just recently\nposted on this exact topic. And I said\nthat, you know, essentially developers\nwill have to adapt by the end of this\nyear. And I've gotten a lot of hate for\nthis video to say the least. Yeah. Yeah.\nMakes sense. Makes sense because people\nare saying essentially that, you know,\nAI can't work on large code bases. Like\nit sure can build you a tetras game, but\nit it doesn't know how to work with\nlarge code bases. And I'm arguing that\nthe reason it doesn't know how to do\nthat is because our code bases are not\nmade for AI. It's not because of AI.\nIt's because of our code bases. So the\npractice that you've just said is\nactually extremely interesting to me.\nLike you add special like links inside\nthe files so they can like navigate\nbetween different documentation pages.\nIs there anything else that you can\nshare for developers and development\nteams on how they can adapt to AI? I now\nhave a lot of cloud files like scattered\naround my repository that explain things\nlike how I do testing. I have a lot of\ncursor rules like I have a lot of cursor\nrules on how I do code review, right?\nIt's like I make cloud code up its PRs.\nSo I say like okay make this big change.\nDon't make one PR, make three PRs, each\none merging to the other one. And now\nwhat I do is I'll run the job. I can go\nget my coffee. And as I'm in line for\ncoffee, I'll get a message that's like,\noh, PR1 has been created. It has only 60\nlines of code. And then what? PR2 has\nbeen created. It's also 60 lines of\ncode. And another 60 lines of code. And\neach pull request now is really easy to\nreview, right? Because we all hate this\nlike thousandline, you know, mega pull\nrequest that some engineer makes. And\nnow Claude can sort of organize that for\nme and write much more detailed\ndescriptions. That has been one thing\nthat to me has been really valuable,\nespecially because cloud code can also\npull the PR comments from GitHub back\ninto the context. So I will be on my\nphone leaving comments in the code on\nGitHub and I'll come back to my desk and\nI just say slashpr comment, pull\neverything back, fix everything I said\nand I go to my next coffee in the coffee\nshop. That's actually one of the things\nthat excites me about AI agents so much\nis that once you've built an AI agent,\nit's basically infinitely scalable.\nLike, you know, you can just tell one\ndeveloper to create two PRs to the same\ncodebase at the same time, right? But\nit's not the case with AI agents. You\nbuild an AI agent once and then it can\ncreate a thousand PRs for you, right? At\nthe same time, it can run in parallel as\nmuch as you want. So when AI brings such\ntremendous scalability to organizations,\nhow do you actually price for a solution\nlike this? Cuz you can't even price it\nas like an employee anymore because it's\nlike a thousand employees, right? Yeah.\nBut again, I think this is why we are\ngoing to move to a world where most\nthings are going to be on value based\npricing. Like would I pay $2,000 for a\nmarketing AI? Would I pay a 10%\ncommission to a marketing AI? And these\nare the questions I think that are going\nto be really important in the future.\nYou know, like if I have an AI book\nmeetings for me, should I pay when the\nmeeting is booked or should I pay when a\nsale is made? Right? There are people\nwho get paid on value based pricing. And\nI think for the most part, people who\nare being paid value based, their\ncustomers are happier cuz everything is\nalignment. And I don't even think it's a\nfavor that you some people charge others\nhourly. I don't think it's a good\nbusiness model to begin with. And I'm\njust really excited to see more and more\nAI systems go into a world where you\nmight be priced on success rather than\npriced on usage. So you also have a\ncourse called systematically improving\nrag applications which from what I've\nheard so far is excellent. We're\nactually considering purchasing it for\nour team as well. Can you briefly\nexplain what does it mean to\nsystematically improve rag applications?\nYeah. So what has happened in the last\nyear was a lot of my consulting was it's\nnot working, it doesn't feel good like\nwhat should we do? And after like six or\nseven engagements, I kind of came up\nwith a playbook of just the steps you\nhave to follow. And now it's kind of\nlike the playbook I use when these\nconsulting engagements sort of involve\nimproving search systems. Step number\none, you create position and recall\nevaluations for your search system. Step\nnumber two, once you have enough evals,\nyou can think about fine-tuning a model.\nBut you can only fine-tune a model if\nyou have feedback data. So let's go now\nevaluate and audit the systems you have\nin place to collect user feedback,\nthumbs up, thumbs down, customer\ncompliance, etc. And by doing that, you\ncan improve your embedding models and\nimprove your search systems. And then\nthe second half of the course is around\ntool discovery, right? What tools does\nmy agent need? How do I figure out if a\ntool is missing? You have to do this\ntopic modeling. Once you find these\ntopics, you can solve each individual\ntopic with a couple of tools. And then\nit's about how do you make sure that\nit's using the tools correctly, right?\nLike for example, do you give your AI\ntools for GitHub or do you just say,\nwell, it has access to command line.\nShouldn't it be able to do everything?\nBut it turns out if you give it a GitHub\ntool, it will make more pull requests.\nIt will create more issues. But if you\ngive it a command line tool, it will use\nother things to get the job done. And so\nit's about breaking up the problem,\nsolving each individual problem\nseparately, and then figure out how to\ncombine them. And those are the two\nsystems I implement when I do my\nconsulting work. Wow, that actually\nsounds really exciting to me. So you\nactually deploy agents without like all\nof the sets that of tools that they\nmight need first. Is that what you're\nproposing is like you deploy an agent,\nyou test it, and then you evaluate and\nthen you add more tools accordingly. Is\nthat how you do it? Exactly. Exactly.\nCuz you know, you want something out in\nthe world and there's just things you\ncan't do and that's usually fine. But if\nyou discover that, you know, 10% or 30%\nof your customers are asking for\nsomething, shouldn't you discover that\nusing the data that you have, right?\nObviously, you want to talk to people,\nbut if you don't have that luxury,\nlooking at the data is the second best\nthing. I think because it's so different\nfrom how we approach traditional\nworkflows because you know if when\nbuilding workflows you just define all\nof the steps in advance you know like\nall of the actions that you can take but\nhere it seems like you're taking a lot\nmore dynamic approach. Yeah, because\nthink about like how we start a\nbusiness, right? We first start a\nbusiness and we go, okay, h what do I do\nnext? Okay, I have to go record videos\nand edit stuff. Okay, well then what?\nAnd then you do that and then what? And\nat some point when you scale, you say,\nyou know what, I've realized that every\ntime I make a video, I should come up\nwith a blog post and I should do this\nand I do that. But it's because you've\ndone it so many times that you've\ndiscovered the subprocess, the standard\noperating procedure that you need to\nimplement in your business. And\nsometimes it's the CEO's job to realize\nthat this thing is happening. Sometimes\nit's the employees job to go, \"Oh, I\nshould automate this in some way.\" And\ndoing it with AI should feel the same\nway. And regarding the first part of the\ncourse, you also mentioned fine-tuning.\nDo you think fine-tuning is necessary or\ndo you start without fine-tuning? I\nthink fine-tuning a language model is\ngoing to be very hard and generally not\ngoing to be worth it mostly because\nowning inference is going to be very\ndifficult. fine-tuning rerankers,\nfine-tuning embedding models for search\nare very worth it. They're going to cost\nyou three or four dollars to train a\nmodel and it's probably going to give\nyou 10 15% better performance. I see.\nCuz I thought you meant traditional fine\ntuning and what I find it's actually\nit's nowhere near as useful as\nfine-tuning your rag application. I also\nsee that the dev tools for AI agent\ndevelopers are just becoming so good\nthat most of us don't even like need to\nthink about this anymore at least for\nsmall businesses where you know like for\nexample open AAI already has the file\nsearch out the box and now like Google\nis also entering the space with like\ntheir own ADK and also their own even\nagent marketplace. Yeah. So, do you\nstill think that like in the future as\nthis space becomes more and more\nproductized and more and more companies\nrelease those amazing dev tools that\nmake agents so much more accessible,\nwill there still be space for\nindependent developers like us or is big\ntech just going to dominate agent\nbuilding completely? I think again it's\ngoing to be the case that for these\nlarge companies they're going to have to\nbuild fairly horizontal tools and so it\nis just very possible to find like\nuseful niches you know like is it going\nto be easy for a developer to make a $10\nmillion company maybe it's going to be\nhard but I can't imagine why you\ncouldn't build a company that's like one\nor two people making like one or $2\nmillion per person I think it's actually\nvery possible now especially with the\nleverage that you have and also because\nI would only say maybe 304 40% of\nrunning a business is the code, right?\nLike you have to become more\nentrepreneurial and more like individual\nwhen it comes to these kind of things.\nSo just taking initiative and focusing\non a specific vertical that those large\ncompanies can't possibly cover by\nthemselves. We're not talking like you\nknow software engineering that's kind of\nobvious. We're talking like some niche\ndown solution that requires like some\nform of a specific expertise and like\nactually you know making it happen like\ntaking initiative and going for it right\nyeah exactly so you're making over six\nfigures per month now doing consulting\nin AI right yeah yeah so for anyone just\ngetting started in this space how do you\nrecommend they find their first client I\nmean I think a lot of it is just figure\nout what interested in and understand\nwhat kind of problems and pains you run\ninto. Right? Once you have that in mind,\nyou can just build your own tools. You\ncan just write about them. For example,\nand free content generally is the\neasiest way to get started. I mean, I\nthink the easiest way really is getting\nreferrals. Like for example, I've\nactually been in machine learning for\nabout 10 years. And so when AI popped up\nin this past couple of years, I was\nalready pretty well established in the\ncommunity. But most of my work now comes\nfrom my writing, right? I am writing\nabout problems that other big companies\nare facing today. Were there any\nspecific mindset shifts that led you to\nbecome you know a leader in AI from just\na developer? I think the big one is\nunderstanding that I have to write about\nthings from the perspective of the pain\nof the customer. If I want to get\ncustomers I can't write about you know\noh this weekend I learned like this\nthing or that thing. It's how you can\nimprove your business right? It is it is\nthe mistakes you need to avoid versus\njust oh I learned this this week or I\nlearned this that week you know those\nthings really nobody cares about right\nin the sense that unless like your\nfollowers could care but maybe not your\ncustomers and so really understanding\nthe pain and the benefit of what your\nsolution is rather than oh how valuable\nam I how smart am I those things really\nput you in a position where you I think\nfor the most part limit your earning\npotential right if I feel like I am a\nsenior engineer and I deserve $250 an\nhour, I know exactly how much I want to\nmake it in the year. But if I have two\ncustomers and one of them has a team of\nseven people or one of them has no team,\nthe team of seven people probably has a\nbigger problem. They might not know how\nto spend their time. And if you waste\nthe time of four engineers, that's\nalready a million dollars a year that\nyou're wasting. So what's 2% of that,\nright? I see. So how exactly do you\nprice your services?\nYeah. So, let me take a step back, which\nis the first thing I try to do is I try\nto have a minimum level of engagement,\nright? It's a number I have on my\nwebsite. And the goal isn't to\ncommunicate, oh, I'm very expensive,\nlike you can't afford me. But neither is\nit to say, you can afford me cuz I'm\nvery cheap. But I say typically\ncustomers tend to spend about $60,000 to\n$80,000 over a period of 2 or 3 months.\nAnd you know, that number on the website\nsort of helps me filter out like what\nkind of customers I was looking for. If\nWalmart goes on my website and they say\n$60,000 like, oh, what could I possibly\nget for that little money, right? I am\ninvesting, you know, $200 million in AI,\nMcKenzie will charge me 34 million, but\nI'm probably going to get a lot more.\nBut a serious, you know, seed company\nthat just raised a million, they might\ngo, \"Oh, I don't actually know what I\nwant, so I don't know if this is a good\nbet.\" So that helps you filter out the\nright customers. And then once you do\nthat, a lot of it is a conversation on\nhow big is the problem that you're\nworking on and how accountable you are\nfor that solution. Okay. So once you've\nfiltered all of those initial customers\nand once you scheduled a call with\nsomeone Yeah. who is okay with this\ninitial price, how do you actually\njustify it to them? So a lot of it is\nunderstanding what the real pain is,\nright? So for example, they might come\nto me and say, Jason, are we need AI to\nbe more reliable? And it's like why now?\nyou know what is at stake in your\nbusiness that you need this AI to be\nmore reliable. A bad customer might say,\n\"I don't know. I've been trying it out.\nIt doesn't seem like it's doing what I\nwant it to do.\" Other customers might\nsay, \"Well, we launched our product. We\nwent viral. We hit a million ARR and the\nchurn is through the roof. We're losing\n20% of our customers every month.\" And\nthen if I ask the question, oh, how much\ndoes that cost to you? Now it says, \"Oh,\nactually I'm losing $200,000 of ARR\nevery month and I don't know if I lose\nthis customer if they're going to come\nback in 3 months. How much is that worth\nto you?\" Right? That sounds like a\nproblem. That's $100,000 every month. If\nI ask for $30,000 a month to stop the\nbleeding, maybe that feels like the\nright kind of deal. And maybe I have\nsome guarantees on like if we don't hit\nthese KPIs, you don't pay me the full\nprice. I see. So a lot of people when\nthey are starting out they're kind of\nthinking about how to price their\nservice. So they're thinking you know\nabout the price in terms of the service\nthat they can provide. Yeah. While what\nyou're saying I think is a lot closer to\nsome form of like a value based pricing\nwhere exactly for the value that you\nprovide. Yeah. And obviously it's not\nalways going to be the case that I can\njust say well I want 20% or 30% of the\nvalue. But I think what this unlocks in\nterms of a mindset is you'll realize how\noften you're not overpricing yourself,\nright? Like here's a story. This company\npaid me $700 an hour to interview their\nsenior AI team and to help them hire the\nhead of AI. So at the end of this, I\nmade\n$20,000. It's like, oh my god, I got\npaid $20,000 to just read people's\nresumes and ask them like what the\nhardest problem they're working on. Like\nwow, lucky me. At the end of that, we\nhired four engineers. Each one was being\npaid $200,000 a year. And we hired a\nhead of AI who had recognized me on a\npodcast, right? And then I find out the\nrecruiter made $200,000 because the\nrecruiter makes\n20% of the payroll of each person. Okay.\nSo, the recruiter made 10 times more\nthan I did, right? Wow. Then with the\nhead of AI joining this company, 6\nmonths later, I see the news. They raise\na series B and the head of AI is like\njoining this company and they raised a\nseries B to like make more investments\nin AI and that was a $20 million like\nseries B. And so the head of AI's equity\nprobably went up quite a bit. You know,\nthe CEO's equity went up probably like a\ncouple million dollars. And I'm sitting\nthere like 6 months later going like,\n\"Oh, I I thought I was like winning\nbecause I was making $700 an hour,\nright?\" Right? But you realize in that\nsituation, I was still the most\nexploited person in that engagement,\nright? The employees made more money.\nThe employees got jobs. The head of AI\njoined the company. They were able to\nraise the money. The recruiter made like\n$200,000. The the founders made like a\nmillion dollars. I say, \"Okay, I need to\nreally think about how I change pricing\nand what they're getting by hire working\nwith me, not how much I cost.\"\nOkay, that sounds really exciting. So\nwhat's on the road map for instructor\nand what are you personally planning to\nfocus on in the future? For the road map\nof instructor, there's not much to be\ndone right now. I think most of it is\nactually going to be around like what\nare the new like LLM tools I plan on\ninvesting in. And I think what we talked\nabout earlier is going to be really\nimportant, right? We want to invest in\nsome more tools to do this data\nanalysis. So we came out with a library\ncalled Cura, Kur A, and that does this\ntopic analysis. And then our goal is to\nalso come out with something that is\ngoing to be sort of a light version of\nRagus to create an eval library. And\nthen what we'll be able to do is say\nokay given these conversations I have\nmetrics and I have ways of grouping the\ndata. So let me group the data find the\naverage metrics and just now allow me to\njust explore more and more what is going\non in these conversations with AI and\nwhat can be done to make them better.\nRight? And I just think that the\nconversations we have with AI now is\nprobably the most interesting data set I\ncan imagine having access to.\nDefinitely. I mean Chad already knows\nmore about me than my parents. Exactly.\nExactly. Exactly. So this is basically\nlike a library for exactly what you just\ndescribed. It's like clustering data,\nextracting insights and seeing how you\ncan further improve your AI agents based\non that. Yeah. Is that correct? Exactly.\nIs it already out or is it just coming\nup? Uh Cura is out. We're mostly working\non the documentation now. Um, I can like\nsend you a link afterwards. We're still\nuh doing some tests, but we'll have a\npresentation on uh AI and engineering\nsummit. Okay, awesome. I'd be happy to\ncover it on one of my videos on this\nchannel later. Yeah, I would love to\nhave Ivan come on and share what he's\nworking on. Yeah, that would be great\nbecause honestly, EVAs has always been\nsuch a challenge for us. Like OpenAI has\ntheir own solution, but it's like a bit\nchallenging to set up. like there's no\nout ofthe-box solution like you\ndescribed. Exactly. Exactly. So, I'm\npretty excited for these things to exist\nand again it's going to be just using\nthose tools sort of drive consulting\nrevenue and then the idea is that these\ntools can like just stay free and stay\nlightweight. All right. So, where can we\nfind out more about your work and learn\nfrom you? You can just find me on\nTwitter at\njxnlco. But generally, if you want to\nlearn more about rag, we have a\nnewsletter called\nimproving.com. And if you want to learn\nabout consulting and you know the work\nthat we do, check out learnnind\nconsulting.com. Two little websites that\ncould help you out. Awesome. Thanks,\nJason. We'll leave all the links down\nbelow and looking forward to covering\nthat library that you mentioned. Yeah.\nAwesome. We'll send an intro afterwards.\nThank their", "language": "en-US"}