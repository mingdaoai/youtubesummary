{"transcript": "Is it possible to steal a million-dollar\napp using AI? This app makes $24 million\nyearly by letting users track calories\nwith photos of their food. But what if I\ntold you it wasn't built by a massive\nteam, but a group of teenagers who are\nstill in school? Today, I'm going to\nattempt something bold. I will rebuild\nthis entire app from scratch using\nnothing but AI tools. So, can today's AI\ntools really help regular people steal\nmillion-dollar apps? Let's find out. In\nstep one, we'll create our master plan.\nStep two, we'll build the app with AI\nusing new tools like DeepSeek and MCP.\nAnd step three, we'll deploy the app to\nApp Store. But first, time for an\nupgrade. This is the voice I will be\nusing from now on. And yes, this is my\nreal voice. Step one, project setup with\nBrain Dumper. First, head to brain\ndumper.ai and choose Windsurf. This is a\ntool I've built to help create the\nproject context, making it easier to\nbuild the app. Here, just brain dump\neverything you want to include in your\napp. When you're done, click next.\nSelect mobile app and then generate\nplan. Once it's done, you'll see this\npage with several download buttons.\nFirst, download the context file. This\ncontains your entire project plan. I've\nlinked it with my referral code, which\ngives you 500 free credits. Or hey, if\nyou don't want the free credits, no\nworries. Then, you need to download\nNodeJS, and we need this to access the\nMPM and MPX commands. Simply explained,\njust to install different packages. And\nfinally, the secret weapon for stealing\nprofessional UI, Mobin. It's basically a\nmassive library of successful mobile\napps broken down screen by screen.\nNormally, if you wanted to study an\napp's design, you'd have to download it,\nclick through every screen, and manually\ntake screenshots. And trust me, that\nprocess sucks. Now, let's search for\ncalories to see if there are any apps to\ncopy here. Okay, nice. So, they have\napps like Livom, My Fitness Pal, and\nMacAractor. I personally like Livom's UI\nthe most, so let's use that. Click on\nflows, and check this out. We can see\nthe entire user journey. Look at their\nonboarding. 31 screens. They've clearly\noptimized this flow to convert users.\nNow, let's copy this entire flow. Click\nthe copy button with the Figma logo,\nthen download plug-in. We'll redirect to\nFigma. Click open in new file. Figma\nfile. Save it. Open mobin again. Click\ninside this box and paste with CtrlV.\nNice. Step two is building the app. Now,\nopen Windsorf. Create a new project\nfolder and drag in your context file\nfrom Brain Dumper. And then let's add\nsome rules to make our build process\nsmoother. Click the Windsor settings at\nthe bottom right corner, then memories\nand rules and manage. Click edit\nworkspace rules. I've prepared the\nwindsurf rules in the video description.\nCopy and paste those in. These rules\ntell windsurf exactly how it should use\nexpo to build mobile apps. Now press ctr\nL to open cascade. This is where we talk\ndirectly to the AI. And let's tell it\nset up the mobile app from the context\nfile using this command. MPX create expo\napp at latest E with router. I'll\nprovide the UI and onboarding user flow\nfrom Figma after the project is set up.\nClick accept when it asks for\npermission. And while this runs, let me\nexplain what Expo router is. This\npicture from Expo's blog explains it\neasily. It just connects all the\ndifferent screens in our app using a\nfile-based routing system. Now, the AI\nwants to install the required libraries,\nso just accept this, too. For some\nreason, Cascade always gets the syntax\nwrong when using multiple commands in\none line, but it's not a problem since\nit corrects itself immediately. Now,\nCascade wants to install more packages\nlike Expo Camera for taking food\npictures, image picker for uploads, and\nExpo notifications. Let's accept. Now,\nit's creating our app structure, folders\nfor onboarding, authentication, home,\ncomponents, and pages. Accept each\nfolder creation one by one. We've got a\nfew errors to fix before we can run the\napp. This is normal in development, but\nwith AI, we can solve these in seconds\ninstead of hours on Stack Overflow. Just\nclick send to Cascade, and the AI will\nunderstand the issues and fix them\nautomatically. Great. Now, let's run the\napp with NPX Expo Start and see what\nwe've got. In your terminal, you'll see\na QR code. Here's what to do next. Grab\nyour phone, download the Expo Go app,\nand scan the QR code from your camera.\nLook at this. We already have the\nonboarding screens. Let's go through\nthem. We've got the goal selection,\ngender input, height, weight, and\nactivity level. All working perfectly\nand even the payw wall screen is built.\nLet's continue to the dashboard. For\nnow, the main dashboard shows\nplaceholder data. Even the profile\nscreen looks good, too. Let's try the\nfood capture feature. Nice. It's already\nasking for camera permissions. But here,\nwe got an error. Let's head back to the\nterminal in Windsurf. Copy the error and\nask Cascade to fix it. Refresh the app\nby pressing R in the terminal. Or you\ncan actually shake your phone and click\nreload. Perfect. Now I can take a\npicture, analyze it, and get nutrition\ndata. Even though it's just placeholder\ndata until we connect the AI\nfunctionality. Now let's add a real\nbackend with Superbase to store all our\nuser data and food logs. Let's ask\nCascade to start setting up the\nSuperbase back end. Starting with the\nuser authentication. When it's done, go\nto superbase.com, sign in, and create a\nnew project. Give it a name, set the\ndatabase password, choose your region,\nand create. Now just wait about 30\nseconds until it's set up. Once\nSuperbase is done, go to project\nsettings API. Copy the URL and API key.\nThen paste them into our ENV file in\nWindsurf. Now we need to set up our\ndatabase tables. Let's ask Cascade to\ngenerate the SQL schema for user\nprofiles, meal tracking, and\nsubscriptions. Copy this code. Go to\nSuperbase SQL editor, paste it in, and\nclick run. Now, if we check table\neditor, we can see all our tables, daily\ngoals, food items, meals, profiles, and\nsubscriptions. Let's now test our\ndatabase connection in the app. Create\nan account and log in. And it works. Now\ncheck Superbase. There's our user in the\ndatabase. Our profile information is\nsaved in the profiles table as well.\nLet's scan a meal and save it. Perfect.\nLet's check the database to confirm\neverything saved properly. Looking at\nthe meals table. Perfect. Our entry is\nthere. And in the food items table, we\ncan see all individual items properly\nstored as well. Now to the exciting\npart. Setting up the Google Cloud Vision\nAPI to make our AI food scanning feature\nactually work. First, grab an API key.\nThen, go to\nconsole.cloud.google.com and create an\naccount. Click my first project, then\nnew project. Name your project and hit\ncreate. Once created, search for API and\nselect API and services. Then, search\nfor cloud vision and select the cloud\nvision API. Finally, click enable. Now\njust navigate to the credentials page\nand click create credentials and choose\nAPI key. And just like that, the API key\nis made. Now copy it and close out of\nthis dialogue window. If you now edit\nthis API key, we can add a good name for\nit. I'll name it calorie snap. Hit save.\nAnd here can actually reveal the key if\nyou need to see it later. With the API\nkey copied, navigate back to Windserve\nand create a completely new chat to\nstart clean. Here we need to let Cascade\nknow that we are using the Google Cloud\nVision API for the AI image scanning. So\nlet's just prompt this. Let's use Google\nCloud Vision API as the AI integration\nof the food scanning. Help me set this\nup. Now let Windsurf generate the code\nfor us. When it's done, we need to\ninsert the API key in the ENV file. So\nopen the MV file and add the placeholder\nfor the Google Cloud API key and paste\nin the actual API key there. Now open\nthe terminal and run npx expo start- c\nto run the app. Scan the QR code again.\nI've tested our app by uploading an\nimage of an apple and it correctly\nshowed no protein and approximately 100\ncalories, confirming that the AI\nimplementation is working. We now have a\ncomplete MVP up and running in under 10\nminutes. But let's now focus on\nimproving the UI and the user experience\nof the app. Earlier we exported all\nthese designs from Mobin to Figma. But\nnow let's quickly create an even better\nfile with only our favorite screenshots.\nI want to take inspiration from these\nthree apps in Mobin. Livesome, My\nFitness Pal, and Macrofactor. So what\nI'll do is speed up this process of me\nspending the next 15 minutes going over\nall these three apps in Mobin and save\nall the best pages. Let's navigate to\nthe saved section and open the\ncollection. As you can see, I've saved\n84 pages in here. What you can do is\ndownload all the screens you've saved.\nor if we mark the first screen and then\nhold shift and mark the last screen. We\ncan copy all the screens over to Figma\nlike we did before. I'm only adding half\nof the screens first because 84 images\nis a lot to process at a time. Now click\ndownload plugin and it will redirect\nover to Figma where we can create a new\nfile. Hit run and then click inside this\nbox and press Ctrl +V to paste in the\ndesigns from Mobin. I'll need to\ncontinue by adding the remaining 42\nscreens. I'll just speed up this part as\nwell. All right, nice. So here we now\nhave a file with all these screens.\nLet's now use these designs as\ninspiration using Windsurf's new\nbuilt-in Figma MCP. Could you explain\nwhat MCP actually is so I understand it?\nYes, of course. So AI coding assistants\nlike Windsurf are limited to just\ngenerating code. If you need to interact\nwith any web pages, databases, or APIs,\nyou're on your own. But that's where MCP\nor model context protocols come in. All\nit is is a standard that lets AI models\ncommunicate with apps and tools more\neasily. Think of MCP as a universal\ntranslator between AI assistants and\ndigital tools. It creates a shared\nlanguage that helps AI understand and\ncontrol services in the real world. Now\nlet's continue. So if we now go back to\nWindsurf and click on Windsurf settings\nin the bottom right corner and then\nclick on advanced settings under\nCascade, we can see the line for MCP.\nClick on add server. And here we can\nalready see that Windinssurf has added\nintegrations with quite a few companies\nalready like Stripe, Slack, and Figma.\nIf we now click on add server for Figma,\nthen we need to add our Figma API key.\nSo, navigate back to Figma and click on\nthis button in the top left corner, help\nan account, and click on account\nsettings. Then, navigate to the security\ntab. And if you now scroll down to\npersonal access tokens, this is where we\ncan create API keys in Figma. Click on\ngenerate new token and I'll name it\ncalorie snap. Back in Windsurf, let's\npaste in this API key and save\nconfiguration. Now we have connected\nWindsurf with Figma using their MCP. And\nthis means that Windinsurf can see all\nthe screens. So let's prompt Cascade to\nlook at the Figma file with the 84\nmobile app screens and use them as\ndesign inspiration to improve the UI and\nUX of our current app. Now the AI will\nlook through all the screen designs from\nthe file we've made and implement a\ncombination of the three app designs in\nour app. After some time, the AI is\nfinally done examining the designs and\nit lists up all the improvements. Now,\nlet's ask it to continue by improving\nspecifically the dashboard page. I'll\nspeed up this process. Okay, now it's\ndone. Let's ask it to improve the other\npages as well. This final prompt\nactually took almost 10 minutes, so this\nhas to be good. Stop the running local\nhost by pressing Ctrl C. And now, let's\nrerun the app with npx expo start- C to\nopen the project freshly. Scan the QR\ncode to open the app. And okay, this\nlooks like it's made by a professional\ndesigner. And all this with just one\nprompt. AI is getting crazy. Just look\nat this. We've actually built this whole\napp with the food scanning feature\nworking perfectly. Superbase handling\nall our data and user accounts. And\nhonestly, the UI is looking super clean.\nNow for the exciting part, let's launch\nthis app onto the app store. And trust\nme, it's way easier than you probably\nthink. We've been playing with it in\nExpo Go on our phones, but now it's time\nto push it to production so your friends\nand potential users can actually\ndownload and test it themselves. I'll\njump over to my MacBook for this part,\nbut of course this is also possible to\ndo on Windows. First, we need an Expo\nEIS account. Head over to\nexpo.dev/signup and create one here.\nYou'll also need an Apple developer\naccount, which costs $99 per year. Go to\ndeveloper.apple.com, click on account,\nand create yours. Now, back in Windsurf,\nlet's type in EAS login. Here you need\nto log into the Expo account you just\nmade. This is to create the project.\nWould you like to create a project? Hit\nY for yes. Nice. Now, the project is\nmade. Now, let's do the final command.\nnpx test flight. This one command will\nbuild the entire app and submit it. It\nneeds to install EAS. Choose yes. Now we\nneed to log in to the Apple account.\nAfter you've logged in, hit yes on all\nthe next questions. Then it continues to\nrun all the commands for you. And now\nit's building the app. This takes a few\nminutes. So just wait until it's done.\nWhen it's done building the app, it will\nalso submit it. Okay. Now let's navigate\nto app store\nconnect.apple.com. Go to apps. And here\nit is. Open it and navigate to\ntestflight. Nice. Let's now create a\nlink for our friends to test the app.\nPress the blue button under external\ntesting and create a new group. Then we\nneed to add the build. So click here,\nselect the build, and press add. Now\nopen the group and create a public link.\nConfirm. You can copy this link and give\nit to your friends to try out the app.\nAnd there it is, a calorie counter app\nbuilt completely from scratch in under\nan hour. Now let's quickly put our\ncreation side by side with those copycat\napps cluttering the app store. Ours\nactually looks just as good as many of\nthose apps. But here's the real\nquestion. Why is Cali so successful when\nthere are dozens of similar apps already\nout there? The honest answer is that\nthey've mastered their marketing\nstrategy. I'll dive deeper into that in\nfuture videos. But think about this.\nWhen Cali first launched, it wasn't a\nfully polished app. It started as a\nsimple chat GPT rapper, a basic minimum\nviable product designed purely to test\nif people even wanted it. Cali turned a\nsimple chat GPT rapper into a\nmulti-million dollar business. Today's\nAI tools give you everything you need to\nrapidly create powerful MVPs to validate\nyour own ideas quickly. And of course,\nthis isn't a complete app yet, but it's\na good MVP to test the market built in\nless than an hour. If you want to see\nexactly how Sax started Cali and built\nit into what it is today, check out this\nvideo", "language": "en-US", "title": "How Is This Legal? Cloning a $24M App With AI"}